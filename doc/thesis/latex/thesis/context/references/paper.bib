%
% paper.bib
%
% inproceedings - An article in a conference proceedings.
%   Required: author, title, booktitle, year.
%   Optional fields: editor, volume/number, series, pages, address, month, organization, publisher, note.

% Reference
% http://en.wikibooks.org/wiki/LaTeX/Bibliography_Management

% -----------------------------------------------------------------------------
@inproceedings{asher-etal-2016-discourse,
    title = "Discourse Structure and Dialogue Acts in Multiparty Dialogue: the {STAC} Corpus",
    author = "Asher, Nicholas  and
      Hunter, Julie  and
      Morey, Mathieu  and
      Farah, Benamara  and
      Afantenos, Stergos",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Grobelnik, Marko  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, Helene  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1432",
    pages = "2721--2727",
    abstract = "This paper describes the STAC resource, a corpus of multi-party chats annotated for discourse structure in the style of SDRT (Asher and Lascarides, 2003; Lascarides and Asher, 2009). The main goal of the STAC project is to study the discourse structure of multi-party dialogues in order to understand the linguistic strategies adopted by interlocutors to achieve their conversational goals, especially when these goals are opposed. The STAC corpus is not only a rich source of data on strategic conversation, but also the first corpus that we are aware of that provides full discourse structures for multi-party dialogues. It has other remarkable features that make it an interesting resource for other topics: interleaved threads, creative language, and interactions between linguistic and extra-linguistic contexts.",
}
% -----------------------------------------------------------------------------
@inproceedings{xu-etal-2020-dialog,
  title     = {Enhancing Dialog Coherence with Event Graph Grounded Content Planning},
  author    = {Xu, Jun and Lei, Zeyang and Wang, Haifeng and Niu, Zheng-Yu and Wu, Hua and Che, Wanxiang},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI-20}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Christian Bessiere},
  pages     = {3941--3947},
  year      = {2020},
  month     = {7},
  note      = {Main track},
  doi       = {10.24963/ijcai.2020/545},
  url       = {https://doi.org/10.24963/ijcai.2020/545},
}
% -----------------------------------------------------------------------------
@inproceedings{marcu-1997-discourse,
    title = "From discourse structures to text summaries",
    author = "Marcu, Daniel",
    booktitle = "Intelligent Scalable Text Summarization",
    year = "1997",
    url = "https://aclanthology.org/W97-0713",
}
% -----------------------------------------------------------------------------
@inproceedings{barzilay-lee-2004-catching,
    title = "Catching the Drift: Probabilistic Content Models, with Applications to Generation and Summarization",
    author = "Barzilay, Regina  and
      Lee, Lillian",
    booktitle = "Proceedings of the Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics: {HLT}-{NAACL} 2004",
    month = may # " 2 - " # may # " 7",
    year = "2004",
    address = "Boston, Massachusetts, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N04-1015",
    pages = "113--120",
}
% -----------------------------------------------------------------------------
@inproceedings{barzilay-lapata-2005-modeling,
    title = "Modeling Local Coherence: An Entity-Based Approach",
    author = "Barzilay, Regina  and
      Lapata, Mirella",
    editor = "Knight, Kevin  and
      Ng, Hwee Tou  and
      Oflazer, Kemal",
    booktitle = "Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05)",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P05-1018",
    doi = "10.3115/1219840.1219858",
    pages = "141--148",
}
% -----------------------------------------------------------------------------
@inproceedings{li-hovy-2014-model,
    title = "A Model of Coherence Based on Distributed Sentence Representation",
    author = "Li, Jiwei  and
      Hovy, Eduard",
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1218",
    doi = "10.3115/v1/D14-1218",
    pages = "2039--2048",
}
% -----------------------------------------------------------------------------
@inproceedings{cohan-etal-2018-discourse,
    title = "A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents",
    author = "Cohan, Arman  and
      Dernoncourt, Franck  and
      Kim, Doo Soon  and
      Bui, Trung  and
      Kim, Seokhwan  and
      Chang, Walter  and
      Goharian, Nazli",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2097",
    doi = "10.18653/v1/N18-2097",
    pages = "615--621",
    abstract = "Neural abstractive summarization models have led to promising results in summarizing relatively short documents. We propose the first model for abstractive summarization of single, longer-form documents (e.g., research papers). Our approach consists of a new hierarchical encoder that models the discourse structure of a document, and an attentive discourse-aware decoder to generate the summary. Empirical results on two large-scale datasets of scientific papers show that our model significantly outperforms state-of-the-art models.",
}
% -----------------------------------------------------------------------------
@inproceedings{dong-etal-2021-discourse,
    title = "Discourse-Aware Unsupervised Summarization for Long Scientific Documents",
    author = "Dong, Yue  and
      Mircea, Andrei  and
      Cheung, Jackie Chi Kit",
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.93",
    doi = "10.18653/v1/2021.eacl-main.93",
    pages = "1089--1102",
    abstract = "We propose an unsupervised graph-based ranking model for extractive summarization of long scientific documents. Our method assumes a two-level hierarchical graph representation of the source document, and exploits asymmetrical positional cues to determine sentence importance. Results on the PubMed and arXiv datasets show that our approach outperforms strong unsupervised baselines by wide margins in automatic metrics and human evaluation. In addition, it achieves performance comparable to many state-of-the-art supervised approaches which are trained on hundreds of thousands of examples. These results suggest that patterns in the discourse structure are a strong signal for determining importance in scientific articles.",
}
% -----------------------------------------------------------------------------
@article{pasunuru-etal-2021-data,
author = {Pasunuru, Ramakanth and Asli, Celikyilmaz and Galley, Michel and Xiong, Chenyan and Zhang, Yizhe and Bansal, Mohit and Gao, Jianfeng},
year = {2021},
month = {05},
pages = {13666-13674},
title = {Data Augmentation for Abstractive Query-Focused Multi-Document Summarization},
volume = {35},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
doi = {10.1609/aaai.v35i15.17611}
}
% -----------------------------------------------------------------------------
@inproceedings{cao-wang-2022-hibrids,
    title = "{HIBRIDS}: Attention with Hierarchical Biases for Structure-aware Long Document Summarization",
    author = "Cao, Shuyang  and
      Wang, Lu",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.58",
    doi = "10.18653/v1/2022.acl-long.58",
    pages = "786--807",
    abstract = "Document structure is critical for efficient information consumption. However, it is challenging to encode it efficiently into the modern Transformer architecture. In this work, we present HIBRIDS, which injects Hierarchical Biases foR Incorporating Document Structure into attention score calculation. We further present a new task, hierarchical question-summary generation, for summarizing salient content in the source document into a hierarchy of questions and summaries, where each follow-up question inquires about the content of its parent question-summary pair. We also annotate a new dataset with 6,153 question-summary hierarchies labeled on government reports. Experiment results show that our model produces better question-summary hierarchies than comparisons on both hierarchy quality and content coverage, a finding also echoed by human judges. Additionally, our model improves the generation of long-form summaries from long government reports and Wikipedia articles, as measured by ROUGE scores.",
}
% -----------------------------------------------------------------------------
@inproceedings{ghosal-etal-2019-dialoguegcn,
    title = "{D}ialogue{GCN}: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
    author = "Ghosal, Deepanway  and
      Majumder, Navonil  and
      Poria, Soujanya  and
      Chhaya, Niyati  and
      Gelbukh, Alexander",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1015",
    doi = "10.18653/v1/D19-1015",
    pages = "154--164",
    abstract = "Emotion recognition in conversation (ERC) has received much attention, lately, from researchers due to its potential widespread applications in diverse areas, such as health-care, education, and human resources. In this paper, we present Dialogue Graph Convolutional Network (DialogueGCN), a graph neural network based approach to ERC. We leverage self and inter-speaker dependency of the interlocutors to model conversational context for emotion recognition. Through the graph network, DialogueGCN addresses context propagation issues present in the current RNN-based methods. We empirically show that this method alleviates such issues, while outperforming the current state of the art on a number of benchmark emotion classification datasets.",
}
% -----------------------------------------------------------------------------
@inproceedings{ouyang-etal-2021-dialogue,
    title = "Dialogue Graph Modeling for Conversational Machine Reading",
    author = "Ouyang, Siru  and
      Zhang, Zhuosheng  and
      Zhao, Hai",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.279",
    doi = "10.18653/v1/2021.findings-acl.279",
    pages = "3158--3169",
}
% -----------------------------------------------------------------------------
@article{yan-etal-2024-dialgnn,
author={Yan, Yan
and Zhang, Bo-Wen
and Min, Peng-hao
and Ding, Guan-wen
and Liu, Jun-yuan},
title={DialGNN: Heterogeneous Graph Neural Networks for Dialogue Classification},
journal={Neural Processing Letters},
year={2024},
month={Apr},
day={08},
volume={56},
number={2},
pages={143},
abstract={Dialogue systems have attracted growing research interests due to its widespread applications in various domains. However, most research work focus on sentence-level intent recognition to interpret user utterances in dialogue systems, while the comprehension of the whole documents has not attracted sufficient attention. In this paper, we propose DialGNN, a heterogeneous graph neural network framework tailored for the problem of dialogue classification which takes the entire dialogue as input. Specifically, a heterogeneous graph is constructed with nodes in different levels of semantic granularity. The graph framework allows flexible integration of various pre-trained language representation models, such as BERT and its variants, which endows DialGNN with powerful text representational capabilities. DialGNN outperforms on CM and ECS datasets, which demonstrates robustness and the effectiveness. Specifically, our model achieves a notable enhancement in performance, optimizing the classification of document-level dialogue text. The implementation of DialGNN and related data are shared through https://github.com/821code/DialGNN.},
issn={1573-773X},
doi={10.1007/s11063-024-11595-z},
url={https://doi.org/10.1007/s11063-024-11595-z}
}
% -----------------------------------------------------------------------------
@article{li-etal-2021-dadgraph,
  title={DADgraph: A Discourse-aware Dialogue Graph Neural Network for Multiparty Dialogue Machine Reading Comprehension},
  author={Jiaqi Li and Ming Liu and Zihao Zheng and Heng Zhang and Bing Qin and Min-Yen Kan and Ting Liu},
  journal={2021 International Joint Conference on Neural Networks (IJCNN)},
  year={2021},
  pages={1-8},
  url={https://api.semanticscholar.org/CorpusID:233394035}
}
% -----------------------------------------------------------------------------
@inproceedings{feng-etal-2021-dialogue,
    author = {Feng, Xiachong and Feng, Xiaocheng and Geng, Xinwei},
    year = {2021},
    month = {08},
    pages = {3808-3814},
    booktitle = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence (IJCAI-21)},
    title = {Dialogue Discourse-Aware Graph Model and Data Augmentation for Meeting Summarization},
    doi = {10.24963/ijcai.2021/524}
}
% -----------------------------------------------------------------------------
@inproceedings{denecke-matthias-2002-rapid,
    title = "Rapid Prototyping for Spoken Dialogue Systems",
    author = "Denecke and Matthias",
    booktitle = "{COLING} 2002: The 19th International Conference on Computational Linguistics",
    year = "2002",
    url = "https://aclanthology.org/C02-1147",
}
% -----------------------------------------------------------------------------
@inproceedings{schlungbaum-elwert-1996-dialogue,
    author = {Egbert Schlungbaum and Thomas Elwert},
    title = {Dialogue Graphs - a Formal and Visual Specification Technique for Dialogue Modelling},
    booktitle = {Proceedings of the BCS-FACS Workshop on Formal Aspects of the Human Computer Interface (FAC)},
    year = {1996},
    doi = {10.14236/ewic/FAC1996.13}
}
% -----------------------------------------------------------------------------
@inproceedings{agarwal-rajeev-1997-towards,
    title = "Towards a {PURE} Spoken Dialogue System for Information Access",
    author = "Agarwal, Rajeev",
    booktitle = "Interactive Spoken Dialog Systems: Bringing Speech and {NLP} Together in Real Applications",
    year = "1997",
    pages = "90--97",
    url = "https://aclanthology.org/W97-0617",
    doi = "10.5555/1641462.1641479"
}
% -----------------------------------------------------------------------------
@inproceedings{aust-oerder-1995-dialogue,
  author={Harald Aust and Martin Oerder},
  title={{Dialogue control in automatic inquiry systems}},
  year=1995,
  booktitle={Proc. ESCA Workshop on Spoken Dialogue Systems},
  pages={121--124}
}
% -----------------------------------------------------------------------------
@inproceedings{wrnestl-2005-modeling,
  title={Modeling a Dialogue Strategy for Personalized Movie Recommendations},
  author={Pontus W{\"a}rnest{\aa}l},
  year={2005},
  url={https://api.semanticscholar.org/CorpusID:51943445}
}
% -----------------------------------------------------------------------------
@article{gritta-etal-2021-conversation,
    author = {Gritta, Milan and Lampouras, Gerasimos and Iacobacci, Ignacio},
    title = "{Conversation Graph: Data Augmentation, Training, and Evaluation for Non-Deterministic Dialogue Management}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {36-52},
    year = {2021},
    month = {02},
    abstract = "{Task-oriented dialogue systems typically rely on large amounts of high-quality training data or require complex handcrafted rules. However, existing datasets are often limited in size con- sidering the complexity of the dialogues. Additionally, conventional training signal in- ference is not suitable for non-deterministic agent behavior, namely, considering multiple actions as valid in identical dialogue states. We propose the Conversation Graph (ConvGraph), a graph-based representation of dialogues that can be exploited for data augmentation, multi- reference training and evaluation of non- deterministic agents. ConvGraph generates novel dialogue paths to augment data volume and diversity. Intrinsic and extrinsic evaluation across three datasets shows that data augmentation and/or multi-reference training with ConvGraph can improve dialogue success rates by up to 6.4\\%.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00352},
    url = {https://doi.org/10.1162/tacl\_a\_00352},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00352/1923936/tacl\_a\_00352.pdf},
}
% -----------------------------------------------------------------------------
@ARTICLE{scarselli-etal-2009-gnn,
  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal={IEEE Transactions on Neural Networks}, 
  title={The Graph Neural Network Model}, 
  year={2009},
  volume={20},
  number={1},
  pages={61-80},
  keywords={Neural networks;Biological system modeling;Data engineering;Computer vision;Chemistry;Biology;Pattern recognition;Data mining;Supervised learning;Parameter estimation;Graphical domains;graph neural networks (GNNs);graph processing;recursive neural networks},
  doi={10.1109/TNN.2008.2005605}}
% -----------------------------------------------------------------------------
@article{hobbs-1979-coherence,
title = {Coherence and coreference},
journal = {Cognitive Science},
volume = {3},
number = {1},
pages = {67-90},
year = {1979},
issn = {0364-0213},
url = {https://www.sciencedirect.com/science/article/pii/S0364021379800439},
author = {Jerry R. Hobbs},
abstract = {Coherence in conversations and in texts can be partially characterized by a set of coherence relations, motivated ultimately by the speaker's or writer's need to be understood. In this paper, formal definitions are given for several coherence relations, based on the operations of an inference system; that is, the relations between successive portions of a discourse are characterized in terms of the inferences that can be drawn from each. In analyzing a discourse, it is frequently the case that we would recognize it as coherent, in that it would satisfy the formal definition of some coherence relation, if only we could assume certain noun phrases to be coreferential. In such cases, we will simply assume the identity of the entities referred to, in what might be called a “petty conversational implicature,” thereby solving the coherence and coreference problems simultaneously. Three examples of different kinds of reference problems are presented. In each, it is shown how the coherence of the discourse can be recognized, and how the reference problems are solved, almost as a by-product, by means of these petty conversational implicatures.}
}
% -----------------------------------------------------------------------------
@inproceedings{hobbs-1985-coherence,
  title={On the coherence and structure of discourse},
  author={Jerry R. Hobbs},
  year={1985},
  url={https://api.semanticscholar.org/CorpusID:118745326}
}
% -----------------------------------------------------------------------------
@inproceedings{zhang-etal-2018-personalizing,
    title = "Personalizing Dialogue Agents: {I} have a dog, do you have pets too?",
    author = "Zhang, Saizheng  and
      Dinan, Emily  and
      Urbanek, Jack  and
      Szlam, Arthur  and
      Kiela, Douwe  and
      Weston, Jason",
    editor = "Gurevych, Iryna  and
      Miyao, Yusuke",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1205",
    doi = "10.18653/v1/P18-1205",
    pages = "2204--2213",
    abstract = "Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i)condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors.",
}
% -----------------------------------------------------------------------------
@InProceedings{dinan-etal-2019-convai2,
author="Dinan, Emily
and Logacheva, Varvara
and Malykh, Valentin
and Miller, Alexander
and Shuster, Kurt
and Urbanek, Jack
and Kiela, Douwe
and Szlam, Arthur
and Serban, Iulian
and Lowe, Ryan
and Prabhumoye, Shrimai
and Black, Alan W.
and Rudnicky, Alexander
and Williams, Jason
and Pineau, Joelle
and Burtsev, Mikhail
and Weston, Jason",
editor="Escalera, Sergio
and Herbrich, Ralf",
title="The Second Conversational Intelligence Challenge (ConvAI2)",
booktitle="The NeurIPS '18 Competition",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="187--208",
abstract="We describe the setting and results of the ConvAI2 NeurIPS competition that aims to further the state-of-the-art in open-domain chatbots. Some key takeaways from the competition are: (1) pretrained Transformer variants are currently the best performing models on this task, (2) but to improve performance on multi-turn conversations with humans, future systems must go beyond single word metrics like perplexity to measure the performance across sequences of utterances (conversations)---in terms of repetition, consistency and balance of dialogue acts (e.g. how many questions asked vs. answered).",
isbn="978-3-030-29135-8"
}
% -----------------------------------------------------------------------------
@article{wolf-etal-2019-trans,
  title={Transfertransfo: A transfer learning approach for neural network based conversational agents},
  author={Wolf, Thomas and Sanh, Victor and Chaumond, Julien and Delangue, Clement},
  journal={arXiv preprint arXiv:1901.08149},
  year={2019},
  booktitle = {CAI Workshop NeurIPS 2018}
}

% -----------------------------------------------------------------------------
@inproceedings{song-etal-2021-bob,
    title = "{B}o{B}: {BERT} Over {BERT} for Training Persona-based Dialogue Models from Limited Personalized Data",
    author = "Song, Haoyu  and
      Wang, Yan  and
      Zhang, Kaiyan  and
      Zhang, Wei-Nan  and
      Liu, Ting",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.14",
    doi = "10.18653/v1/2021.acl-long.14",
    pages = "167--177",
    abstract = "Maintaining a consistent persona is essential for dialogue agents. Although tremendous advancements have been brought, the limited-scale of annotated personalized dialogue datasets is still a barrier towards training robust and consistent persona-based dialogue models. This work shows how this challenge can be addressed by disentangling persona-based dialogue generation into two sub-tasks with a novel BERT-over-BERT (BoB) model. Specifically, the model consists of a BERT-based encoder and two BERT-based decoders, where one decoder is for response generation, and another is for consistency understanding. In particular, to learn the ability of consistency understanding from large-scale non-dialogue inference data, we train the second decoder in an unlikelihood manner. Under different limited data settings, both automatic and human evaluations demonstrate that the proposed model outperforms strong baselines in response quality and persona consistency.",
}
% -----------------------------------------------------------------------------
@inproceedings{liu-etal-2020-impress,
    title = "You Impress Me: Dialogue Generation via Mutual Persona Perception",
    author = "Liu, Qian  and
      Chen, Yihong  and
      Chen, Bei  and
      Lou, Jian-Guang  and
      Chen, Zixuan  and
      Zhou, Bin  and
      Zhang, Dongmei",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.131",
    doi = "10.18653/v1/2020.acl-main.131",
    pages = "1417--1427",
    abstract = "Despite the continuing efforts to improve the engagingness and consistency of chit-chat dialogue systems, the majority of current work simply focus on mimicking human-like responses, leaving understudied the aspects of modeling understanding between interlocutors. The research in cognitive science, instead, suggests that understanding is an essential signal for a high-quality chit-chat conversation. Motivated by this, we propose P{\^{}}2 Bot, a transmitter-receiver based framework with the aim of explicitly modeling understanding. Specifically, P{\^{}}2 Bot incorporates mutual persona perception to enhance the quality of personalized dialogue generation. Experiments on a large public dataset, Persona-Chat, demonstrate the effectiveness of our approach, with a considerable boost over the state-of-the-art baselines across both automatic metrics and human evaluations.",
}
% -----------------------------------------------------------------------------
@inproceedings{williams-etal-2018-broad,
    title = "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",
    author = "Williams, Adina  and
      Nangia, Nikita  and
      Bowman, Samuel",
    editor = "Walker, Marilyn  and
      Ji, Heng  and
      Stent, Amanda",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1101",
    doi = "10.18653/v1/N18-1101",
    pages = "1112--1122",
    abstract = "This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.",
}
% -----------------------------------------------------------------------------
@inproceedings{li-etal-2016-diversity,
    title = "A Diversity-Promoting Objective Function for Neural Conversation Models",
    author = "Li, Jiwei  and
      Galley, Michel  and
      Brockett, Chris  and
      Gao, Jianfeng  and
      Dolan, Bill",
    editor = "Knight, Kevin  and
      Nenkova, Ani  and
      Rambow, Owen",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1014",
    doi = "10.18653/v1/N16-1014",
    pages = "110--119",
}
% -----------------------------------------------------------------------------
@inproceedings{li-etal-2016-persona,
    title = "A Persona-Based Neural Conversation Model",
    author = "Li, Jiwei  and
      Galley, Michel  and
      Brockett, Chris  and
      Spithourakis, Georgios  and
      Gao, Jianfeng  and
      Dolan, Bill",
    editor = "Erk, Katrin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1094",
    doi = "10.18653/v1/P16-1094",
    pages = "994--1003",
}
% -----------------------------------------------------------------------------
@inproceedings{jang-etal-2022-focus,
  title={Call for Customized Conversation: Customized Conversation Grounding Persona and Knowledge},
  author={Jang, Yoonna and Lim, Jungwoo and Hur, Yuna and Oh, Dongsuk and Son, Suhyune and Lee, Yeonsoo and Shin, Donghoon and Kim, Seungryong and Lim, Heuiseok},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={10},
  pages={10803--10812},
  year={2022}
}
% -----------------------------------------------------------------------------
@article{alrfou-etal-2016-conversational,
  title={Conversational contextual cues: The case of personalization and history for response ranking},
  author={Al-Rfou, Rami and Pickett, Marc and Snaider, Javier and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
  journal={arXiv preprint arXiv:1606.00372},
  year={2016}
}
% -----------------------------------------------------------------------------
@inproceedings{chen-etal-2023-memorize,
author = {Chen, Ruijun and Wang, Jin and Yu, Liang-Chih and Zhang, Xuejie},
title = {Learning to memorize entailment and discourse relations for persona-consistent dialogues},
year = {2023},
isbn = {978-1-57735-880-0},
publisher = {AAAI Press},
url = {https://doi.org/10.1609/aaai.v37i11.26489},
doi = {10.1609/aaai.v37i11.26489},
abstract = {Maintaining engagement and consistency is particularly important in dialogue systems. Existing works have improved the performance of dialogue systems by intentionally learning interlocutor personas with sophisticated network structures. One issue with this approach is that it requires more personal corpora with annotations. Additionally, these models typically perform the next utterance prediction to generate a response but neglect the discourse coherence in the entire conversation. To address these issues, this study proposes a method of learning to memorize entailment and discourse relations for persona-consistent dialogue tasks. Entailment text pairs in natural language inference dataset were applied to learn latent entailment relations as external memories by premise-to-hypothesis generation task. Furthermore, an internal memory with a similar architecture was applied to the discourse information in the dialogue. Placing orthogonality restrictions on these two memory spaces ensures that the latent entailment relations remain dialogue-independent. Both memories collaborate to obtain entailment and discourse representation for the generation, allowing a deeper understanding of both consistency and coherence. Experiments on two large public datasets, PersonaChat and DSTC7-AVSD, demonstrated the effectiveness of the proposed method. Both automatic and human evaluations indicate that the proposed model outperforms several strong baselines in terms of both persona consistency and response coherence.},
booktitle = {Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence},
articleno = {1420},
numpages = {9},
series = {AAAI'23/IAAI'23/EAAI'23}
}
% -----------------------------------------------------------------------------
@inproceedings{huang-etal-2023-paa,
  title={Personalized dialogue generation with persona-adaptive attention},
  author={Huang, Qiushi and Zhang, Yu and Ko, Tom and Liu, Xubo and Wu, Bo and Wang, Wenwu and Tang, H},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={11},
  pages={12916--12923},
  year={2023}
}
% -----------------------------------------------------------------------------
@inproceedings{bao-etal-2020-plato,
    title = "{PLATO}: Pre-trained Dialogue Generation Model with Discrete Latent Variable",
    author = "Bao, Siqi  and
      He, Huang  and
      Wang, Fan  and
      Wu, Hua  and
      Wang, Haifeng",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.9",
    doi = "10.18653/v1/2020.acl-main.9",
    pages = "85--96",
    abstract = "Pre-training models have been proved effective for a wide range of natural language processing tasks. Inspired by this, we propose a novel dialogue generation pre-training framework to support various kinds of conversations, including chit-chat, knowledge grounded dialogues, and conversational question answering. In this framework, we adopt flexible attention mechanisms to fully leverage the bi-directional context and the uni-directional characteristic of language generation. We also introduce discrete latent variables to tackle the inherent one-to-many mapping problem in response generation. Two reciprocal tasks of response generation and latent act recognition are designed and carried out simultaneously within a shared network. Comprehensive experiments on three publicly available datasets verify the effectiveness and superiority of the proposed framework.",
}
% -----------------------------------------------------------------------------
@inproceedings{zhang-etal-2019-dialogpt,
    title={DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation},
    author={Yizhe Zhang and Siqi Sun and Michel Galley and Yen-Chun Chen and Chris Brockett and Xiang Gao and Jianfeng Gao and Jingjing Liu and Bill Dolan},
    year={2020},
    booktitle={ACL, system demonstration}
}
% -----------------------------------------------------------------------------
@article{liu-etal-2023-disenttangled, title={A Disentangled-Attention Based Framework with Persona-Aware Prompt Learning for Dialogue Generation}, volume={37}, url={https://ojs.aaai.org/index.php/AAAI/article/view/26556}, DOI={10.1609/aaai.v37i11.26556}, abstractNote={Endowing dialogue agents with personas is the key to delivering more human-like conversations. However, existing persona-grounded dialogue systems still lack informative details of human conversations and tend to reply with inconsistent and generic responses. One of the main underlying causes is that pre-defined persona sentences are generally short and merely superficial descriptions of personal attributes, making appropriate persona selection and understanding non-trivial. Another challenge is that it is crucial to consider the context and the conversation flow to dynamically determine when to invoke different types of persona signals. To address these problems, we propose a disentangled-attention based pre-training architecture, which incorporates persona-aware prompt learning to bridge the connection between the selected persona and response generation. Our model first exploits the conversation flow to select context-relevant personas, and subsequently enriches the superficial persona descriptions with extra personality traits through persona-aware prompting. Finally, the decoder leverages a disentangled-attention mechanism to flexibly control the reliance on personas and dialogue contexts, and incorporates A*-like keyword-based heuristic estimates for controllable generation. Extensive experiments show that our approach can outperform strong baselines and deliver more consistent and engaging responses on the PERSONA-CHAT dataset.}, number={11}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Liu, Pingsheng and Huang, Zhengjie and Zhang, Xiechi and Wang, Linlin and de Melo, Gerard and Lin, Xin and Pang, Liang and He, Liang}, year={2023}, month={Jun.}, pages={13255-13263} }
% -----------------------------------------------------------------------------
@inproceedings{zhao-etal-2017-learning,
    title = "Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders",
    author = "Zhao, Tiancheng  and
      Zhao, Ran  and
      Eskenazi, Maxine",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1061",
    doi = "10.18653/v1/P17-1061",
    pages = "654--664",
    abstract = "While recent neural encoder-decoder models have shown great promise in modeling open-domain conversations, they often generate dull and generic responses. Unlike past work that has focused on diversifying the output of the decoder from word-level to alleviate this problem, we present a novel framework based on conditional variational autoencoders that capture the discourse-level diversity in the encoder. Our model uses latent variables to learn a distribution over potential conversational intents and generates diverse responses using only greedy decoders. We have further developed a novel variant that is integrated with linguistic prior knowledge for better performance. Finally, the training procedure is improved through introducing a bag-of-word loss. Our proposed models have been validated to generate significantly more diverse responses than baseline approaches and exhibit competence of discourse-level decision-making.",
}
% -----------------------------------------------------------------------------
@inproceedings{tang-etal-2023-enhancing-personalized,
    title = "Enhancing Personalized Dialogue Generation with Contrastive Latent Variables: Combining Sparse and Dense Persona",
    author = "Tang, Yihong  and
      Wang, Bo  and
      Fang, Miao  and
      Zhao, Dongming  and
      Huang, Kun  and
      He, Ruifang  and
      Hou, Yuexian",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.299",
    doi = "10.18653/v1/2023.acl-long.299",
    pages = "5456--5468",
    abstract = "The personalized dialogue explores the consistent relationship between dialogue generation and personality. Existing personalized dialogue agents model persona profiles from three resources: sparse or dense persona descriptions and dialogue histories. However, sparse structured persona attributes are explicit but uninformative, dense persona texts contain rich persona descriptions with much noise, and dialogue history query is both noisy and uninformative for persona modeling. In this work, we combine the advantages of the three resources to obtain a richer and more accurate persona. We design a Contrastive Latent Variable-based model (CLV) that clusters the dense persona descriptions into sparse categories, which are combined with the history query to generate personalized responses. Experimental results on Chinese and English datasets demonstrate our model{'}s superiority in personalization.",
}
% -----------------------------------------------------------------------------
@inproceedings{lee-etal-2021-dlvgen,
  title={DLVGen: A Dual Latent Variable Approach to Personalized Dialogue Generation},
  author={Jing Yang Lee and Kong Aik LEE and W. S. Gan},
  booktitle={International Conference on Agents and Artificial Intelligence},
  year={2021},
  url={https://api.semanticscholar.org/CorpusID:244478160}
}
% -----------------------------------------------------------------------------
@inproceedings{song-etal-2019-exploiting,
  title     = {Exploiting Persona Information for Diverse Generation of Conversational Responses},
  author    = {Song, Haoyu and Zhang, Wei-Nan and Cui, Yiming and Wang, Dong and Liu, Ting},
  booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages     = {5190--5196},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/721},
  url       = {https://doi.org/10.24963/ijcai.2019/721},
}
% -----------------------------------------------------------------------------
@inproceedings{shen-etal-2017-conditional,
    title = "A Conditional Variational Framework for Dialog Generation",
    author = "Shen, Xiaoyu  and
      Su, Hui  and
      Li, Yanran  and
      Li, Wenjie  and
      Niu, Shuzi  and
      Zhao, Yang  and
      Aizawa, Akiko  and
      Long, Guoping",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-2080",
    doi = "10.18653/v1/P17-2080",
    pages = "504--509",
    abstract = "Deep latent variable models have been shown to facilitate the response generation for open-domain dialog systems. However, these latent variables are highly randomized, leading to uncontrollable generated responses. In this paper, we propose a framework allowing conditional response generation based on specific attributes. These attributes can be either manually assigned or automatically detected. Moreover, the dialog states for both speakers are modeled separately in order to reflect personal features. We validate this framework on two different scenarios, where the attribute refers to genericness and sentiment states respectively. The experiment result testified the potential of our model, where meaningful responses can be generated in accordance with the specified attributes.",
}
% -----------------------------------------------------------------------------
@article{serban-etal-2017-hierarchical, title={A Hierarchical Latent Variable Encoder-Decoder Model for Generating Dialogues}, volume={31}, url={https://ojs.aaai.org/index.php/AAAI/article/view/10983}, DOI={10.1609/aaai.v31i1.10983}, abstractNote={ &lt;p&gt; Sequential data often possesses hierarchical structures with complex dependencies between sub-sequences, such as found between the utterances in a dialogue. To model these dependencies in a generative framework, we propose a neural network-based generative architecture, with stochastic latent variables that span a variable number of time steps. We apply the proposed model to the task of dialogue response generation and compare it with other recent neural-network architectures. We evaluate the model performance through a human evaluation study. The experiments demonstrate that our model improves upon recently proposed models and that the latent variables facilitate both the generation of meaningful, long and diverse responses and maintaining dialogue state. &lt;/p&gt; }, number={1}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Serban, Iulian and Sordoni, Alessandro and Lowe, Ryan and Charlin, Laurent and Pineau, Joelle and Courville, Aaron and Bengio, Yoshua}, year={2017}, month={Feb.} }
% -----------------------------------------------------------------------------
@inproceedings{lu-etal-2023-miracle,
    title = "Miracle: Towards Personalized Dialogue Generation with Latent-Space Multiple Personal Attribute Control",
    author = "Lu, Zhenyi  and
      Wei, Wei  and
      Qu, Xiaoye  and
      Mao, Xian-Ling  and
      Chen, Dangyang  and
      Chen, Jixiong",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.395",
    doi = "10.18653/v1/2023.findings-emnlp.395",
    pages = "5933--5957",
    abstract = "Personalized dialogue systems aim to endow the chatbot agent with more anthropomorphic traits for human-like interactions. Previous approaches have explored explicitly user profile modeling using text descriptions, implicit derivation of user embeddings, or utilizing handicraft prompts for ChatGPT-like models. However, textual personas are limited in describing multi-faceted attributes (\textit{e.g.}, \textit{language style, inner character nuances}), implicit embedding suffers from personality sparsity, and handicraft prompts lack fine-grained and stable controllability. Hence, these approaches may struggle with complex personalized dialogue generation tasks that require generating controllable responses with multiple personal attributes. To this end, we propose \textbf{Miracle}, a novel personalized dialogue generation method through \textbf{M}ult\textbf{I}ple Pe\textbf{R}sonal \textbf{A}ttributes \textbf{C}ontrol within \textbf{L}atent-Space \textbf{E}nergy-based Models. ttributes \textbf{C}ontrol within \textbf{L}atent-Space \textbf{E}nergy-based Models. Specifically, our approach first disentangles complex personality into multi-faceted attributes. Subsequently, we employ a conditional variational auto-encoder to align with the dense personalized responses within a latent joint attribute space. We have also tailored a dedicated energy function and customized the ordinary differential equations sampling method to offer flexible attribute composition and precise attribute control. Extensive experiments demonstrate that Miracle outperforms several strong baselines in terms of personality controllability and response generation quality. Our dataset and code are available at \url{https://github.com/LZY-the-boys/MIRACLE}",
}
% -----------------------------------------------------------------------------
@inproceedings{lester-etal-2021-power,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.243",
    doi = "10.18653/v1/2021.emnlp-main.243",
    pages = "3045--3059",
    abstract = "In this work, we explore {``}prompt tuning,{''} a simple yet effective mechanism for learning {``}soft prompts{''} to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3{'}s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method {``}closes the gap{''} and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed {``}prefix tuning{''} of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient {``}prompt ensembling.{''} We release code and model checkpoints to reproduce our experiments.",
}
% -----------------------------------------------------------------------------
@inproceedings{brown-etal-2020-gpt3,
author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
title = {Language models are few-shot learners},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {159},
numpages = {25},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}
% -----------------------------------------------------------------------------
@inproceedings{bao-etal-2021-plato,
    title = "{PLATO-2}: Towards Building an Open-Domain Chatbot via Curriculum Learning",
    author = "Bao, Siqi  and
      He, Huang  and
      Wang, Fan  and
      Wu, Hua  and
      Wang, Haifeng  and
      Wu, Wenquan  and
      Guo, Zhen  and
      Liu, Zhibin  and
      Xu, Xinchao",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-acl.222",
    doi = "10.18653/v1/2021.findings-acl.222",
    pages = "2513--2525",
}
% -----------------------------------------------------------------------------
@inproceedings{chen-etal-2023-controllable,
    title = "Controllable Mixed-Initiative Dialogue Generation through Prompting",
    author = "Chen, Maximillian  and
      Yu, Xiao  and
      Shi, Weiyan  and
      Awasthi, Urvi  and
      Yu, Zhou",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-short.82",
    doi = "10.18653/v1/2023.acl-short.82",
    pages = "951--966",
    abstract = "Mixed-initiative dialogue tasks involve repeated exchanges of information and conversational control. Conversational agents gain control by generating responses that follow particular dialogue intents or strategies, prescribed by a policy planner. The standard approach has been fine-tuning pre-trained language models to perform generation conditioned on these intents. However, these supervised generation models are limited by the cost and quality of data annotation. We instead prompt large language models as a drop-in replacement to fine-tuning on conditional generation. We formalize prompt construction for controllable mixed-initiative dialogue. Our findings show improvements over fine-tuning and ground truth responses according to human evaluation and automatic metrics for two tasks: PersuasionForGood and Emotional Support Conversations.",
}
% -----------------------------------------------------------------------------
@inproceedings{madotto-etal-2019-personalizing,
    title = "Personalizing Dialogue Agents via Meta-Learning",
    author = "Madotto, Andrea  and
      Lin, Zhaojiang  and
      Wu, Chien-Sheng  and
      Fung, Pascale",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'\i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1542",
    doi = "10.18653/v1/P19-1542",
    pages = "5454--5459",
    abstract = "Existing personalized dialogue models use human designed persona descriptions to improve dialogue consistency. Collecting such descriptions from existing dialogues is expensive and requires hand-crafted feature designs. In this paper, we propose to extend Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) to personalized dialogue learning without using any persona descriptions. Our model learns to quickly adapt to new personas by leveraging only a few dialogue samples collected from the same user, which is fundamentally different from conditioning the response on the persona descriptions. Empirical results on Persona-chat dataset (Zhang et al., 2018) indicate that our solution outperforms non-meta-learning baselines using automatic evaluation metrics, and in terms of human-evaluated fluency and consistency.",
}
% -----------------------------------------------------------------------------
@inproceedings{yuan-etal-2021-bartscore,
 author = {Yuan, Weizhe and Neubig, Graham and Liu, Pengfei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {27263--27277},
 publisher = {Curran Associates, Inc.},
 title = {BARTScore: Evaluating Generated Text as Text Generation},
 url = {https://proceedings.neurips.cc/paper/2021/file/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Paper.pdf},
 volume = {34},
 year = {2021}
}
% -----------------------------------------------------------------------------
@inproceedings{lewis-etal-2020-bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}
% -----------------------------------------------------------------------------
@inproceedings{jiang-de-rijke-2018-sequence,
    title = "Why are Sequence-to-Sequence Models So Dull? Understanding the Low-Diversity Problem of Chatbots",
    author = "Jiang, Shaojie  and
      de Rijke, Maarten",
    editor = "Chuklin, Aleksandr  and
      Dalton, Jeff  and
      Kiseleva, Julia  and
      Borisov, Alexey  and
      Burtsev, Mikhail",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {SCAI}: The 2nd International Workshop on Search-Oriented Conversational {AI}",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5712",
    doi = "10.18653/v1/W18-5712",
    pages = "81--86",
    abstract = "Diversity is a long-studied topic in information retrieval that usually refers to the requirement that retrieved results should be non-repetitive and cover different aspects. In a conversational setting, an additional dimension of diversity matters: an engaging response generation system should be able to output responses that are diverse and interesting. Sequence-to-sequence (Seq2Seq) models have been shown to be very effective for response generation. However, dialogue responses generated by Seq2Seq models tend to have low diversity. In this paper, we review known sources and existing approaches to this low-diversity problem. We also identify a source of low diversity that has been little studied so far, namely model over-confidence. We sketch several directions for tackling model over-confidence and, hence, the low-diversity problem, including confidence penalties and label smoothing.",
}
% -----------------------------------------------------------------------------
@inproceedings{nie-etal-2021-like,
    title = "{I} like fish, especially dolphins: Addressing Contradictions in Dialogue Modeling",
    author = "Nie, Yixin  and
      Williamson, Mary  and
      Bansal, Mohit  and
      Kiela, Douwe  and
      Weston, Jason",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.134",
    doi = "10.18653/v1/2021.acl-long.134",
    pages = "1699--1713",
    abstract = "To quantify how well natural language understanding models can capture consistency in a general conversation, we introduce the DialoguE COntradiction DEtection task (DECODE) and a new conversational dataset containing both human-human and human-bot contradictory dialogues. We show that: (i) our newly collected dataset is notably more effective at providing supervision for the dialogue contradiction detection task than existing NLI data including those aimed to cover the dialogue domain; (ii) Transformer models that explicitly hinge on utterance structures for dialogue contradiction detection are more robust and generalize well on both analysis and out-of-distribution dialogues than standard (unstructured) Transformers. We also show that our best contradiction detection model correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots.",
}
% -----------------------------------------------------------------------------
@inproceedings{brody-etal-2022-gatv2,
  title={How Attentive are Graph Attention Networks? },
  author={Shaked Brody and Uri Alon and Eran Yahav},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/forum?id=F72ximsx7C1}
}
% -----------------------------------------------------------------------------
@article{wu-etal-2023-gnn-pretrain,
author = {Wu, Lirong and Lin, Haitao and Tan, Cheng and Gao, Zhangyang and Li, Stan Z.},
title = {Self-Supervised Learning on Graphs: Contrastive, Generative, or Predictive},
year = {2023},
issue_date = {April 2023},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {35},
number = {4},
issn = {1041-4347},
url = {https://doi.org/10.1109/TKDE.2021.3131584},
doi = {10.1109/TKDE.2021.3131584},
abstract = {Deep learning on graphs has recently achieved remarkable success on a variety of tasks, while such success relies heavily on the massive and carefully labeled data. However, precise annotations are generally very expensive and time-consuming. To address this problem, self-supervised learning (SSL) is emerging as a new paradigm for extracting informative knowledge through well-designed pretext tasks without relying on manual labels. In this survey, we extend the concept of SSL, which first emerged in the fields of computer vision and natural language processing, to present a timely and comprehensive review of existing SSL techniques for graph data. Specifically, we divide existing graph SSL methods into three categories: contrastive, generative, and predictive. More importantly, unlike other surveys that only provide a high-level description of published research, we present an additional mathematical summary of existing works in a unified framework. Furthermore, to facilitate methodological development and empirical comparisons, we also summarize the commonly used datasets, evaluation metrics, downstream tasks, open-source implementations, and experimental study of various algorithms. Finally, we discuss the technical challenges and potential future directions for improving graph self-supervised learning. Latest advances in graph SSL are summarized in a GitHub repository <uri>https://github.com/LirongWu/awesome-graph-self-supervised-learning</uri>.},
journal = {IEEE Trans. on Knowl. and Data Eng.},
month = {apr},
pages = {4216–4235},
numpages = {20}
}
% -----------------------------------------------------------------------------
@inproceedings{dziri-etal-2019-augmenting,
    title = "Augmenting Neural Response Generation with Context-Aware Topical Attention",
    author = "Dziri, Nouha  and
      Kamalloo, Ehsan  and
      Mathewson, Kory  and
      Zaiane, Osmar",
    editor = "Chen, Yun-Nung  and
      Bedrax-Weiss, Tania  and
      Hakkani-Tur, Dilek  and
      Kumar, Anuj  and
      Lewis, Mike  and
      Luong, Thang-Minh  and
      Su, Pei-Hao  and
      Wen, Tsung-Hsien",
    booktitle = "Proceedings of the First Workshop on NLP for Conversational AI",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-4103",
    doi = "10.18653/v1/W19-4103",
    pages = "18--31",
    abstract = "Sequence-to-Sequence (Seq2Seq) models have witnessed a notable success in generating natural conversational exchanges. Notwithstanding the syntactically well-formed responses generated by these neural network models, they are prone to be acontextual, short and generic. In this work, we introduce a Topical Hierarchical Recurrent Encoder Decoder (THRED), a novel, fully data-driven, multi-turn response generation system intended to produce contextual and topic-aware responses. Our model is built upon the basic Seq2Seq model by augmenting it with a hierarchical joint attention mechanism that incorporates topical concepts and previous interactions into the response generation. To train our model, we provide a clean and high-quality conversational dataset mined from Reddit comments. We evaluate THRED on two novel automated metrics, dubbed Semantic Similarity and Response Echo Index, as well as with human evaluation. Our experiments demonstrate that the proposed model is able to generate more diverse and contextually relevant responses compared to the strong baselines.",
}
% -----------------------------------------------------------------------------
@article{kipf-etal-2016-vgae,
  title={Variational Graph Auto-Encoders},
  author={Kipf, Thomas N and Welling, Max},
  journal={NIPS Workshop on Bayesian Deep Learning},
  year={2016}
}
% -----------------------------------------------------------------------------
@inproceedings{zhou-etal-2023-simoap,
    title = "{S}im{OAP}: Improve Coherence and Consistency in Persona-based Dialogue Generation via Over-sampling and Post-evaluation",
    author = "Zhou, Junkai  and
      Pang, Liang  and
      Shen, Huawei  and
      Cheng, Xueqi",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.553",
    doi = "10.18653/v1/2023.acl-long.553",
    pages = "9945--9959",
    abstract = "Language models trained on large-scale corpora can generate remarkably fluent results in open-domain dialogue. However, for the persona-based dialogue generation task, consistency and coherence are also key factors, which are great challenges for language models. Existing works mainly focus on valuable data filtering, model structure modifying, or objective function designing, while their improvements are limited and hard to generalize to all types of pre-trained language models. However, we find that language models can produce consistent and coherent responses if we consider enough generations. Thus, the problems lay in large-scale response generation and target response selection. In this work, a simple but effective two-stage SimOAP strategy is proposed, i.e., over-sampling and post-evaluation. The over-sampling stage takes large-scale responses from existing trained models efficiently via off-the-shelf distilling and compressing methods, and the post-evaluation stage selects a good response based on multiple well-designed evaluation metrics from large-scale candidates. Experimental results show that the proposed plug-in SimOAP strategy improves the backbone models and outperforms the baseline strategies in both automatic and human evaluations.",
}
% -----------------------------------------------------------------------------
@inproceedings{zhang-etal-2018-generating,
 author = {Zhang, Yizhe and Galley, Michel and Gao, Jianfeng and Gan, Zhe and Li, Xiujun and Brockett, Chris and Dolan, Bill},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generating Informative and Diverse Conversational Responses via Adversarial Information Maximization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/23ce1851341ec1fa9e0c259de10bf87c-Paper.pdf},
 volume = {31},
 year = {2018}
}

% -----------------------------------------------------------------------------
@inproceedings{li-etal-2020-generate,
author = {Li, Lei and Zhang, Yongfeng and Chen, Li},
title = {Generate Neural Template Explanations for Recommendation},
year = {2020},
isbn = {9781450368599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340531.3411992},
doi = {10.1145/3340531.3411992},
abstract = {Personalized recommender systems are important to assist user decision-making in the era of information overload. Meanwhile, explanations of the recommendations further help users to better understand the recommended items so as to make informed choices, which gives rise to the importance of explainable recommendation research. Textual sentence-based explanation has been an important form of explanations for recommender systems due to its advantage in communicating rich information to users. However, current approaches to generating sentence explanations are either limited to predefined sentence templates, which restricts the sentence expressiveness, or opt for free-style sentence generation, which makes it difficult for sentence quality control. In an attempt to benefit both sentence expressiveness and quality, we propose a Neural Template (NETE) explanation generation framework, which brings the best of both worlds by learning sentence templates from data and generating template-controlled sentences that comment about specific features. Experimental results on real-world datasets show that NETE consistently outperforms state-of-the-art explanation generation approaches in terms of sentence quality and expressiveness. Further analysis on case study also shows the advantages of NETE on generating diverse and controllable explanations.},
booktitle = {Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
pages = {755–764},
numpages = {10},
keywords = {explainable recommendation, natural language generation, neural template explanation, recommender systems},
location = {Virtual Event, Ireland},
series = {CIKM '20}
}
% -----------------------------------------------------------------------------
@inproceedings{zhang-etal-2020-bert-score,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Tianyi Zhang* and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=SkeHuCVFDr}
}
% -----------------------------------------------------------------------------
@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    editor = "Isabelle, Pierre  and
      Charniak, Eugene  and
      Lin, Dekang",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}
% -----------------------------------------------------------------------------
@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W04-1013",
    pages = "74--81",
}
% -----------------------------------------------------------------------------
 @article{song-etal-2020-generating,
    title={Generating Persona Consistent Dialogues by Exploiting Natural  Language Inference},
    volume={34},
    DOI={10.1609/aaai.v34i05.6417},
    number={05},
    journal={Proceedings of the AAAI Conference on Artificial Intelligence},
    author={Song, Haoyu and Zhang, Wei-Nan and Hu, Jingwen and Liu, Ting},
    year={2020},
    month={Apr.},
    pages={8878-8885}
 }
% -----------------------------------------------------------------------------
@inproceedings{deshpande-etal-2023-toxicity,
    title = "Toxicity in chatgpt: Analyzing persona-assigned language models",
    author = "Deshpande, Ameet  and
      Murahari, Vishvak  and
      Rajpurohit, Tanmay  and
      Kalyan, Ashwin  and
      Narasimhan, Karthik",
    editor = "Bouamor, Houda  and
      Pino, Juan  and
      Bali, Kalika",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2023",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-emnlp.88",
    doi = "10.18653/v1/2023.findings-emnlp.88",
    pages = "1236--1270",
    abstract = "Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Legislation has recognized its significance and recently drafted a {``}Blueprint For An AI Bill Of Rights{''} which calls for domain experts to identify risks and potential impact of AI systems. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to $6\times$, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. Furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others ($3\times$ more) irrespective of the assigned persona, reflecting discriminatory biases in the model. Our findings show that multiple provisions in the legislative blueprint are being violated, and we hope that the broader AI community rethinks the efficacy of current safety guardrails and develops better techniques that lead to robust, safe, and trustworthy AI.",
}
% -----------------------------------------------------------------------------
@inproceedings{reimers-2019-sentence-bert,
  title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
  author = "Reimers, Nils and Gurevych, Iryna",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
  month = "11",
  year = "2019",
  publisher = "Association for Computational Linguistics",
  url = "https://arxiv.org/abs/1908.10084",
}
% -----------------------------------------------------------------------------
@INPROCEEDINGS {cui-etal-2019-cbloss,
author = {Y. Cui and M. Jia and T. Lin and Y. Song and S. Belongie},
booktitle = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Class-Balanced Loss Based on Effective Number of Samples},
year = {2019},
volume = {},
issn = {},
pages = {9260-9269},
abstract = {With the rapid increase of large-scale, real-world datasets, it becomes critical to address the problem of long-tailed data distribution (i.e., a few classes account for most of the data, while most classes are under-represented). Existing solutions typically adopt class re-balancing strategies such as re-sampling and re-weighting based on the number of observations for each class. In this work, we argue that as the number of samples increases, the additional benefit of a newly added data point will diminish. We introduce a novel theoretical framework to measure data overlap by associating with each sample a small neighboring region rather than a single point. The effective number of samples is defined as the volume of samples and can be calculated by a simple-formula (1-βn)/(1-β), where n is the number of samples and β ∈ [0, 1) is a hyperparameter. We design a re-weighting scheme that uses the effective number of samples for each class to re-balance the loss, thereby yielding a class-balanced loss. Comprehensive experiments are conducted on artificially induced long-tailed CIFAR datasets and large-scale datasets including ImageNet and iNaturalist. Our results show that when trained with the proposed class-balanced loss, the network is able to achieve significant performance gains on long-tailed datasets.},
keywords = {},
doi = {10.1109/CVPR.2019.00949},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2019.00949},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}
% -----------------------------------------------------------------------------
@inproceedings{ye-etal-2021-towards-quantifiable,
    title = "Towards Quantifiable Dialogue Coherence Evaluation",
    author = "Ye, Zheng  and
      Lu, Liucun  and
      Huang, Lishan  and
      Lin, Liang  and
      Liang, Xiaodan",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.211",
    doi = "10.18653/v1/2021.acl-long.211",
    pages = "2718--2729",
    abstract = "Automatic dialogue coherence evaluation has attracted increasing attention and is crucial for developing promising dialogue systems. However, existing metrics have two major limitations: (a) they are mostly trained in a simplified two-level setting (coherent vs. incoherent), while humans give Likert-type multi-level coherence scores, dubbed as {``}quantifiable{''}; (b) their predicted coherence scores cannot align with the actual human rating standards due to the absence of human guidance during training. To address these limitations, we propose Quantifiable Dialogue Coherence Evaluation (QuantiDCE), a novel framework aiming to train a quantifiable dialogue coherence metric that can reflect the actual human rating standards. Specifically, QuantiDCE includes two training stages, Multi-Level Ranking (MLR) pre-training and Knowledge Distillation (KD) fine-tuning. During MLR pre-training, a new MLR loss is proposed for enabling the model to learn the coarse judgement of coherence degrees. Then, during KD fine-tuning, the pretrained model is further finetuned to learn the actual human rating standards with only very few human-annotated data. To advocate the generalizability even with limited fine-tuning data, a novel KD regularization is introduced to retain the knowledge learned at the pre-training stage. Experimental results show that the model trained by QuantiDCE presents stronger correlations with human judgements than the other state-of-the-art metrics.",
}
% -----------------------------------------------------------------------------
@inproceedings{ghazarian-etal-2022-deam,
    title = "{DEAM}: Dialogue Coherence Evaluation using {AMR}-based Semantic Manipulations",
    author = "Ghazarian, Sarik  and
      Wen, Nuan  and
      Galstyan, Aram  and
      Peng, Nanyun",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.57",
    doi = "10.18653/v1/2022.acl-long.57",
    pages = "771--785",
    abstract = "Automatic evaluation metrics are essential for the rapid development of open-domain dialogue systems as they facilitate hyper-parameter tuning and comparison between models. Although recently proposed trainable conversation-level metrics have shown encouraging results, the quality of the metrics is strongly dependent on the quality of training data. Prior works mainly resort to heuristic text-level manipulations (e.g. utterances shuffling) to bootstrap incoherent conversations (negative examples) from coherent dialogues (positive examples). Such approaches are insufficient to appropriately reflect the incoherence that occurs in interactions between advanced dialogue models and humans. To tackle this problem, we propose DEAM, a Dialogue coherence Evaluation metric that relies on Abstract Meaning Representation (AMR) to apply semantic-level Manipulations for incoherent (negative) data generation. AMRs naturally facilitate the injection of various types of incoherence sources, such as coreference inconsistency, irrelevancy, contradictions, and decrease engagement, at the semantic level, thus resulting in more natural incoherent samples. Our experiments show that DEAM achieves higher correlations with human judgments compared to baseline methods on several dialog datasets by significant margins. We also show that DEAM can distinguish between coherent and incoherent dialogues generated by baseline manipulations, whereas those baseline models cannot detect incoherent examples generated by DEAM. Our results demonstrate the potential of AMR-based semantic manipulations for natural negative example generation.",
}
% -----------------------------------------------------------------------------

% -----------------------------------------------------------------------------

% -----------------------------------------------------------------------------

% -----------------------------------------------------------------------------

% -----------------------------------------------------------------------------

% -----------------------------------------------------------------------------

% -----------------------------------------------------------------------------

% -----------------------------------------------------------------------------

% -----------------------------------------------------------------------------

% -----------------------------------------------------------------------------

% -----------------------------------------------------------------------------

% -----------------------------------------------------------------------------